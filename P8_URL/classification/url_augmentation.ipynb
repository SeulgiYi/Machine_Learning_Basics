{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Class:  benign\n",
      "Delete 214 of nan data from 35378 of raw data\n",
      "final data set: 35164\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35164"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib import parse \n",
    "import re\n",
    "regex = re.compile(r'(?P<protocol>[\\w]+)\\:\\/\\/(?P<host>[\\w\\.\\-]+)(\\:(?P<port>\\d+))?((?P<path>\\/[^\\?\\#\\n]*)(\\?(?P<params>(?P<first_param>[^\\#\\?\\&\\=\\n]+(=[^\\#\\?\\&\\=\\n]*)*)(\\&(?P<param_more>[^\\#\\?\\&\\=\\n]+(=[^\\#\\?\\&\\=\\n]*)*))*))?(\\#(?P<tag>[^\\#\\?\\n]+)?)?)?$')\n",
    "\n",
    "\n",
    "# get all data used for training \n",
    "class load_data:\n",
    "    def __init__(self):\n",
    "        dir_data = './RawData/FinalDataset/URL' #UNB dataset\n",
    "        #class_name = ['benign', 'defacement', 'malware', 'phishing', 'spam']\n",
    "        class_name = ['benign']\n",
    "        file_list = os.listdir(dir_data)\n",
    "        data = []\n",
    "        for idx, cname in enumerate(class_name):\n",
    "            for file in file_list:\n",
    "                if cname in file.lower():\n",
    "                    file_path = os.path.join(dir_data, file)\n",
    "                    print('==== Class:{:>8s}'.format(cname))\n",
    "                    df_data = self.prepare_data(file_path, idx)\n",
    "                    data.append(self.get_regular_exp_url(df_data, regex))\n",
    "        all_data = pd.concat(data)\n",
    "        self.all_data = all_data.drop(['if_match'], axis=1)\n",
    "        \n",
    "    # trim data\n",
    "    def prepare_data(self, dir_file, class_name):\n",
    "        col_name = ['url', 'class']\n",
    "        urls = pd.read_csv(dir_file, header=None).to_numpy()\n",
    "        values = []\n",
    "        for url in urls:\n",
    "            url = url[0]\n",
    "            #eliminate the unecessary character\n",
    "            if \"'\" in url:\n",
    "                url = url.replace(\"'\", \"\")\n",
    "            values.append([url, class_name])\n",
    "        data = pd.DataFrame(values, columns=col_name) \n",
    "        data = data.replace(\"\", np.nan)\n",
    "        return data\n",
    "\n",
    "    # extract URLs only following regular expression\n",
    "    def get_regular_exp_url(self, df_url, regex):\n",
    "        if_match = df_url['url'].apply(lambda x: self._check_regex(regex, x))\n",
    "        if_match = pd.DataFrame({'if_match':if_match})\n",
    "        data = pd.concat([df_url, if_match], axis=1)\n",
    "        print('Delete {} of nan data from {} of raw data'.format( \n",
    "            data['if_match'].isna().sum(), data.shape[0]))\n",
    "        data = data.dropna(axis=0)\n",
    "        return data\n",
    "    \n",
    "    def _check_regex(self, regex, string):\n",
    "        return regex.match(string)\n",
    "\n",
    "\n",
    "\n",
    "# original data\n",
    "load = load_data()\n",
    "data_org = load.all_data\n",
    "print('final data set: {}'.format(data_org.shape[0]))\n",
    "data_org.head(5)\n",
    "print(data_org['class'].unique())\n",
    "data_org[ data_org['class'] == 0].head(5)\n",
    "data_org['url'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheme 1.0\n",
      "netloc 1.0\n",
      "path 0.9965589807757934\n",
      "params 8.531452621999773e-05\n",
      "query 0.26905357752246617\n",
      "fragment 0.0\n",
      "class 1.0\n",
      "                                                     url  class\n",
      "15581  https://udn.com/Content/AP-In-The-News/2014/AP...      0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        http://io9.com/movies/4576769/jennifer_aniston...\n",
       "1        http://pornsharing.com/magazine/wissen/blog/br...\n",
       "2        https://bdnews24.com/2015/05/12/crystal-palace...\n",
       "3        http://infospace.com/shop/g/men/watch/the-watc...\n",
       "4        http://videomega.tv/jokes/all/page/1/hero/0/ca...\n",
       "                               ...                        \n",
       "19995    https://quizlet.com/post/related/id/554cd79e4a...\n",
       "19996    http://techcrunch.com/torrent/4189296/Sia%2B%2...\n",
       "19997    http://pikabu.ru/questions/9041152/sql-sum-and...\n",
       "19998    http://creativemarket.com/la-cool/nhung-tac-ph...\n",
       "19999    https://abcnews.go.com/2015/04/15/definitely-c...\n",
       "Name: url, Length: 19999, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class data_augmentation:\n",
    "    def __init__(self, data_org, n_add=1):\n",
    "        class_labels = data_org['class'].unique()\n",
    "        data_aug = list()\n",
    "        for label in class_labels:\n",
    "            self.data_current = data_org[ data_org['class'] == label ]\n",
    "            data_frac = self.get_fraction(self.data_current)\n",
    "            bag, freq = self.get_bag(data_frac)\n",
    "            data_aug.append(self.augment_data(bag, freq, n_add, label))\n",
    "        self.data_aug = pd.concat(data_aug) \n",
    "        \n",
    "    def get_fraction(self, df_data):\n",
    "        col_name = ['scheme', 'netloc', 'path', 'params', 'query', 'fragment', 'class']\n",
    "        values = list()\n",
    "        for url, url_class in zip(df_data['url'], df_data['class']):\n",
    "            if '://' not in url:\n",
    "                url = '//' + url\n",
    "            if \"'\" in url:\n",
    "                url = url.replace(\"'\",\"\")\n",
    "            url_parsed = parse.urlparse(url)\n",
    "            values.append([url_parsed.scheme, url_parsed.netloc, \n",
    "                           url_parsed.path, url_parsed.params, url_parsed.query, \n",
    "                           url_parsed.fragment, url_class])\n",
    "        data = pd.DataFrame(values, columns=col_name)\n",
    "        data = data.replace(\"\", np.nan)\n",
    "        data = data.replace(\"/\", np.nan)\n",
    "        return data\n",
    "    \n",
    "    def get_bag(self, data_frac):\n",
    "        bag = dict()\n",
    "        bag['scheme'] = data_frac['scheme'].unique()\n",
    "        bag['netloc'] = data_frac['netloc'].unique()\n",
    "        bag['path'] = data_frac['path'].unique()\n",
    "        param_keys, param_vals = self._get_key_val(data_frac['params'])\n",
    "        query_keys, query_vals = self._get_key_val(data_frac['query'])\n",
    "        bag['param_key'] = param_keys\n",
    "        bag['param_val'] = param_vals\n",
    "        bag['query_key'] = query_keys\n",
    "        bag['query_val'] = query_vals\n",
    "        bag['fragment'] = data_frac['fragment'].unique()\n",
    "        \n",
    "        freq = dict()\n",
    "        for key in data_frac.keys():\n",
    "            freq_frac = data_frac[key].notna().sum()/self.data_current.shape[0]\n",
    "            print( key, freq_frac )\n",
    "            freq[key] = freq_frac\n",
    "        return bag, freq\n",
    "    \n",
    "    def augment_data(self, bag, freq, n_add, label):\n",
    "        data_aug = list()\n",
    "        for i in range(n_add):\n",
    "            scheme = self._pick_one(bag['scheme'], freq['scheme'])\n",
    "            #hostname = self._pick_one(bag['hostname'], 1.0)\n",
    "            #port = self._pick_one(bag['port'], 0.0)\n",
    "            netloc = self._pick_one(bag['netloc'], freq['netloc'])\n",
    "            path = self._pick_one(bag['path'], freq['path'])\n",
    "            \n",
    "            n_param = 3    #random.randrange(0,5)\n",
    "            n_query = 3    #random.randrange(0,5)\n",
    "            param = dict()\n",
    "            query = dict()\n",
    "            for i in range(n_param):\n",
    "                param_key = self._pick_one(bag['param_key'], freq['params'])\n",
    "                if param_key is not 'None':\n",
    "                    #param_val = self._pick_one(bag['param_val'], freq['params'])\n",
    "                    param_val = self._pick_one(bag['param_val'], 0.99)\n",
    "                    param[param_key] = param_val\n",
    "\n",
    "            for i in range(n_query):\n",
    "                query_key = self._pick_one(bag['query_key'], freq['query'])\n",
    "                if query_key is not 'None':\n",
    "                    #query_val = self._pick_one(bag['query_val'], freq['query']) \n",
    "                    query_val = self._pick_one(bag['query_val'], 0.99) \n",
    "                    query[query_key] = query_val\n",
    "            fragment = self._pick_one(bag['fragment'], freq['fragment'])\n",
    "            \n",
    "            url_parsed = parse.urlparse('')\n",
    "            url_new = url_parsed._replace(scheme=self._quote(scheme), \n",
    "                                          netloc=self._quote(netloc), \n",
    "                                          path=self._quote(path), \n",
    "                                          params=self._quote_query(param), \n",
    "                                          query=self._quote_query(query),\n",
    "                                          fragment=self._quote(fragment))\n",
    "            url_new = parse.urlunparse(url_new)\n",
    "            if not param:\n",
    "                url_new = url_new.replace(';','')\n",
    "            if not query:\n",
    "                url_new = url_new.replace('?','')\n",
    "            if fragment is 'None':\n",
    "                url_new = url_new.replace('#','')\n",
    "            \n",
    "            data_aug.append([url_new, label])\n",
    "        data_aug = pd.DataFrame(data_aug, columns=['url','class'])\n",
    "        print(data_aug[ data_aug.duplicated(['url']) ])\n",
    "        data_aug = data_aug.drop_duplicates(subset=['url'])\n",
    "        return data_aug\n",
    "\n",
    "    def _get_key_val(self, df_target):    \n",
    "        keys = list()\n",
    "        vals = list()\n",
    "        for query in df_target.to_numpy():\n",
    "            if query is not np.nan:\n",
    "                for (key, val) in parse.parse_qsl(query):\n",
    "                    keys.append(key)\n",
    "                    vals.append(val)\n",
    "        keys = np.unique(keys, return_counts=False)\n",
    "        vals = np.unique(vals, return_counts=False)\n",
    "        return keys, vals\n",
    "    \n",
    "    def _pick_one(self, component, frequency):\n",
    "               \n",
    "        if random.random() <= frequency :\n",
    "            return component[ random.randrange(0,len(component)) ]\n",
    "        else:\n",
    "            return 'None'\n",
    "    \n",
    "    def _quote_query(self, d):\n",
    "        query = dict()\n",
    "        for key in d.keys():\n",
    "            query[key] = str(d[key]).replace('None','')          \n",
    "        return parse.urlencode(query, doseq=True)\n",
    "\n",
    "    def _quote(self, string):\n",
    "        return parse.quote(str(string).replace('None',''))\n",
    "\n",
    "\n",
    "# augmented data\n",
    "n_add = 20000\n",
    "augment_data = data_augmentation(data_org, n_add=n_add)\n",
    "data_aug = augment_data.data_aug\n",
    "data_aug['url']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== dataset original+augmented\n",
      "train: 54606\n",
      "valid: 0\n",
      "test : 551\n",
      "==== dataset small\n",
      "train: 10922\n",
      "valid: 0\n",
      "test : 111\n",
      "==== dataset mini\n",
      "train: 5461\n",
      "valid: 0\n",
      "test : 56\n"
     ]
    }
   ],
   "source": [
    "# split data: train / valid / test\n",
    "import os\n",
    "\n",
    "dir_save = './augmented'\n",
    "if not os.path.isdir(dir_save):\n",
    "    os.mkdir(dir_save)\n",
    "\n",
    "all_data = pd.concat([data_org, data_aug])\n",
    "all_data = all_data.drop_duplicates(subset=['url'])\n",
    "\n",
    "def split_train_test(data, ratio):\n",
    "    data = data.to_numpy()\n",
    "    n_train = math.ceil(len(data)*ratio[0])\n",
    "    n_valid = math.ceil(len(data)*ratio[1])\n",
    "    indx_shuffle = [ x for x in range(data.shape[0]) ]\n",
    "    random.shuffle(indx_shuffle)\n",
    "    shuffled = data[indx_shuffle]\n",
    "    train = shuffled[:n_train, :]\n",
    "    if n_valid > 0: \n",
    "        valid = shuffled[n_train:n_train+n_valid, :]    \n",
    "    else:\n",
    "        valid = np.empty((n_valid,2))\n",
    "    test = shuffled[n_train+n_valid:, :]\n",
    "    return train, valid, test\n",
    "\n",
    "ratio = [0.99, 0.0, 0.01]\n",
    "train, valid, test = split_train_test(all_data, ratio)\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(train, columns=['url','label'])\n",
    "df_valid = pd.DataFrame(valid, columns=['url','label'])\n",
    "df_test = pd.DataFrame(test, columns=['url','label'])\n",
    "\n",
    "df_train.to_csv(os.path.join(dir_save,'benign_augmented.txt'), columns=['url'], header=None, index=False)\n",
    "df_valid.to_csv(os.path.join(dir_save,'benign_augmented_valid.txt'), columns=['url'], header=None, index=False)\n",
    "df_test.to_csv(os.path.join(dir_save,'benign_augmented_test.txt'), columns=['url'], header=None, index=False)\n",
    "\n",
    "print('==== dataset {}'.format('original+augmented'))\n",
    "print('train: {}'.format(df_train.shape[0]))\n",
    "print('valid: {}'.format(df_valid.shape[0]))\n",
    "print('test : {}'.format(df_test.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "set_ratio = [0.2, 0.1]\n",
    "set_name = ['small', 'mini']\n",
    "\n",
    "for (ratio, sname) in zip(set_ratio, set_name):\n",
    "    n_small_train = math.ceil(len(train)*ratio)\n",
    "    n_small_valid = math.ceil(len(valid)*ratio)\n",
    "    n_small_test = math.ceil(len(test)*ratio)\n",
    "    df_train_small = df_train.head(n_small_train)\n",
    "    df_valid_small = df_valid.head(n_small_valid)\n",
    "    df_test_small = df_test.head(n_small_test)\n",
    "    \n",
    "    df_train_small.to_csv(os.path.join(dir_save,'benign_augmented_{}.txt'.format(sname)), columns=['url'], header=None, index=False)\n",
    "    df_valid_small.to_csv(os.path.join(dir_save,'benign_augmented_{}_valid.txt'.format(sname)), columns=['url'], header=None, index=False)\n",
    "    df_test_small.to_csv(os.path.join(dir_save,'benign_augmented_{}_test.txt'.format(sname)), columns=['url'], header=None, index=False)\n",
    "    \n",
    "    print('==== dataset {}'.format(sname))\n",
    "    print('train: {}'.format(df_train_small.shape[0]))\n",
    "    print('valid: {}'.format(df_valid_small.shape[0]))\n",
    "    print('test : {}'.format(df_test_small.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptlesson]",
   "language": "python",
   "name": "conda-env-ptlesson-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
