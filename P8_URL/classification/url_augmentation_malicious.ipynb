{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Class:defacement\n",
      "Delete 114 of nan data from 96457 of raw data\n",
      "==== Class: malware\n",
      "Delete 252 of nan data from 11566 of raw data\n",
      "==== Class:phishing\n",
      "Delete 782 of nan data from 9965 of raw data\n",
      "==== Class:    spam\n",
      "Delete 1436 of nan data from 12000 of raw data\n",
      "final data set: 127404\n",
      "117314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.sinduscongoias.com.br/index.php/ins...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  class\n",
       "0        http://www.sinduscongoias.com.br/index.html      1\n",
       "1  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "2  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "3  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "4  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "5  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "6  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "7  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "8  http://www.sinduscongoias.com.br/index.php/ins...      1\n",
       "9  http://www.sinduscongoias.com.br/index.php/ins...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib import parse \n",
    "import re\n",
    "regex = re.compile(r'(?P<protocol>[\\w]+)\\:\\/\\/(?P<host>[\\w\\.\\-]+)(\\:(?P<port>\\d+))?((?P<path>\\/[^\\?\\#\\n]*)(\\?(?P<params>(?P<first_param>[^\\#\\?\\&\\=\\n]+(=[^\\#\\?\\&\\=\\n]*)*)(\\&(?P<param_more>[^\\#\\?\\&\\=\\n]+(=[^\\#\\?\\&\\=\\n]*)*))*))?(\\#(?P<tag>[^\\#\\?\\n]+)?)?)?$')\n",
    "\n",
    "\n",
    "# get all data used for training \n",
    "class load_data:\n",
    "    def __init__(self):\n",
    "        dir_data = './RawData/FinalDataset/URL' #UNB dataset\n",
    "        class_name = {'benign': 0, 'defacement': 1, 'malware': 2, 'phishing':3, 'spam':4}\n",
    "        file_list = os.listdir(dir_data)\n",
    "        data = []\n",
    "        for cname, idx in class_name.items():\n",
    "            if idx == 0: \n",
    "                continue\n",
    "            for file in file_list:\n",
    "                if cname in file.lower():\n",
    "                    file_path = os.path.join(dir_data, file)\n",
    "                    print('==== Class:{:>8s}'.format(cname))\n",
    "                    df_data = self.prepare_data(file_path, idx)\n",
    "                    data.append(self.get_regular_exp_url(df_data, regex))\n",
    "        all_data = pd.concat(data)\n",
    "        self.all_data = all_data.drop(['if_match'], axis=1)\n",
    "        \n",
    "    # trim data\n",
    "    def prepare_data(self, dir_file, class_name):\n",
    "        col_name = ['url', 'class']\n",
    "        urls = pd.read_csv(dir_file, header=None).to_numpy()\n",
    "        values = []\n",
    "        for url in urls:\n",
    "            url = url[0]\n",
    "            #eliminate the unecessary character\n",
    "            if \"'\" in url:\n",
    "                url = url.replace(\"'\", \"\")\n",
    "            values.append([url, class_name])\n",
    "        data = pd.DataFrame(values, columns=col_name) \n",
    "        data = data.replace(\"\", np.nan)\n",
    "        return data\n",
    "\n",
    "    # extract URLs only following regular expression\n",
    "    def get_regular_exp_url(self, df_url, regex):\n",
    "        if_match = df_url['url'].apply(lambda x: self._check_regex(regex, x))\n",
    "        if_match = pd.DataFrame({'if_match':if_match})\n",
    "        data = pd.concat([df_url, if_match], axis=1)\n",
    "        print('Delete {} of nan data from {} of raw data'.format( \n",
    "            data['if_match'].isna().sum(), data.shape[0]))\n",
    "        data = data.dropna(axis=0)\n",
    "        return data\n",
    "    \n",
    "    def _check_regex(self, regex, string):\n",
    "        return regex.match(string)\n",
    "\n",
    "\n",
    "\n",
    "# original data\n",
    "load = load_data()\n",
    "data_org = load.all_data\n",
    "print('final data set: {}'.format(data_org.shape[0]))\n",
    "\n",
    "print(data_org['url'].unique().shape[0])\n",
    "data_org.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== class 1: 96343\n",
      "scheme 1.0\n",
      "netloc 1.0\n",
      "path 1.0\n",
      "query 0.569299274467268\n",
      "fragment 1.0379581287690854e-05\n",
      "class 1.0\n",
      "\n",
      "==== class 2: 11314\n",
      "scheme 1.0\n",
      "netloc 1.0\n",
      "path 0.9980555064521831\n",
      "query 0.3136821636910023\n",
      "fragment 0.0\n",
      "class 1.0\n",
      "\n",
      "==== class 3: 9183\n",
      "scheme 1.0\n",
      "netloc 1.0\n",
      "path 0.9603615376238702\n",
      "query 0.1195687683763476\n",
      "fragment 0.002069040618534248\n",
      "class 1.0\n",
      "\n",
      "==== class 4: 10564\n",
      "scheme 1.0\n",
      "netloc 1.0\n",
      "path 0.9995266944339265\n",
      "query 0.48182506626277927\n",
      "fragment 0.0\n",
      "class 1.0\n",
      "\n",
      "==== augmented data: 39997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.prosuzdal.ru/Latest/store-locations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://josedeitx.com/component/content/article...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.caseaffittipuglia.com/it/fotogaller...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://onlineigri.net/all-games/armored-fighte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.leine-net.de/where-we-work/zambia/1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.centromutuitoscana.it/nl/scandia/11...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://tanzaniawerkgroeptilburg.nl/linkpartner...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.heide-marys-windelgesindel.de/templ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://helptheorphans.net/scopri-ciardo.html?v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.ondrejov.cz/services/freight/virusp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  class\n",
       "0  http://www.prosuzdal.ru/Latest/store-locations...      1\n",
       "1  http://josedeitx.com/component/content/article...      1\n",
       "2  http://www.caseaffittipuglia.com/it/fotogaller...      1\n",
       "3  http://onlineigri.net/all-games/armored-fighte...      1\n",
       "4  http://www.leine-net.de/where-we-work/zambia/1...      1\n",
       "5  http://www.centromutuitoscana.it/nl/scandia/11...      1\n",
       "6  http://tanzaniawerkgroeptilburg.nl/linkpartner...      1\n",
       "7  http://www.heide-marys-windelgesindel.de/templ...      1\n",
       "8  http://helptheorphans.net/scopri-ciardo.html?v...      1\n",
       "9  http://www.ondrejov.cz/services/freight/virusp...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class data_augmentation:\n",
    "    def __init__(self, data_org, n_add=1):\n",
    "        class_labels = data_org['class'].unique()\n",
    "        data_aug = list()\n",
    "        for label in class_labels:\n",
    "            self.data_current = data_org[ data_org['class'] == label ]\n",
    "            print('\\n==== class {}: {}'.format(label, self.data_current.shape[0]))\n",
    "            data_frac = self.get_fraction(self.data_current)\n",
    "            bag, freq = self.get_bag(data_frac)\n",
    "            data_aug.append(self.augment_data(bag, freq, n_add, label))\n",
    "        self.data_aug = pd.concat(data_aug) \n",
    "        \n",
    "    def get_fraction(self, df_data):\n",
    "        col_name = ['scheme', 'netloc', 'path', 'query', 'fragment', 'class']\n",
    "        values = list()\n",
    "        for url, url_class in zip(df_data['url'], df_data['class']):\n",
    "            if '://' not in url:\n",
    "                url = '//' + url\n",
    "            if \"'\" in url:\n",
    "                url = url.replace(\"'\",\"\")\n",
    "            url_parsed = parse.urlsplit(url)\n",
    "            values.append([url_parsed.scheme, url_parsed.netloc, \n",
    "                           url_parsed.path, url_parsed.query, \n",
    "                           url_parsed.fragment, url_class])\n",
    "        data = pd.DataFrame(values, columns=col_name)\n",
    "        data = data.replace(\"\", np.nan)\n",
    "        data = data.replace(\"/\", np.nan)\n",
    "        return data\n",
    "    \n",
    "    def get_bag(self, data_frac):\n",
    "        bag = dict()\n",
    "        bag['scheme'] = data_frac['scheme'].unique()\n",
    "        bag['netloc'] = data_frac['netloc'].unique()\n",
    "        bag['path'] = data_frac['path'].unique()\n",
    "        query_keys, query_vals = self._get_key_val(data_frac['query'])\n",
    "        bag['query_key'] = query_keys\n",
    "        bag['query_val'] = query_vals\n",
    "        bag['fragment'] = data_frac['fragment'].unique()\n",
    "        \n",
    "        freq = dict()\n",
    "        for key in data_frac.keys():\n",
    "            freq_frac = data_frac[key].notna().sum()/self.data_current.shape[0]\n",
    "            print( key, freq_frac )\n",
    "            freq[key] = freq_frac\n",
    "        return bag, freq\n",
    "    \n",
    "    def augment_data(self, bag, freq, n_add, label):\n",
    "        data_aug = list()\n",
    "        for i in range(n_add):\n",
    "            scheme = self._pick_one(bag['scheme'], freq['scheme'])\n",
    "            #hostname = self._pick_one(bag['hostname'], 1.0)\n",
    "            #port = self._pick_one(bag['port'], 0.0)\n",
    "            netloc = self._pick_one(bag['netloc'], freq['netloc'])\n",
    "            path = self._pick_one(bag['path'], freq['path'])\n",
    "            n_query = 3    #random.randrange(0,5)\n",
    "            query = dict()\n",
    "            for i in range(n_query):\n",
    "                query_key = self._pick_one(bag['query_key'], freq['query'])\n",
    "                if query_key is not 'None':\n",
    "                    #query_val = self._pick_one(bag['query_val'], freq['query']) \n",
    "                    query_val = self._pick_one(bag['query_val'], 0.99) \n",
    "                    query[query_key] = query_val\n",
    "            fragment = self._pick_one(bag['fragment'], freq['fragment'])\n",
    "            \n",
    "            url_parsed = parse.urlsplit('')\n",
    "            url_new = url_parsed._replace(scheme=self._quote(scheme), \n",
    "                                          netloc=self._quote(netloc), \n",
    "                                          path=self._quote(path),  \n",
    "                                          query=self._quote_query(query),\n",
    "                                          fragment=self._quote(fragment))\n",
    "            url_new = parse.urlunsplit(url_new)\n",
    "            \n",
    "            if not query:\n",
    "                url_new = url_new.replace('?','')\n",
    "            if fragment is 'None':\n",
    "                url_new = url_new.replace('#','')\n",
    "            \n",
    "            data_aug.append([url_new, label])\n",
    "        data_aug = pd.DataFrame(data_aug, columns=['url','class'])\n",
    "        #print(data_aug[ data_aug.duplicated(['url']) ])\n",
    "        data_aug = data_aug.drop_duplicates(subset=['url'])\n",
    "        return data_aug\n",
    "\n",
    "    def _get_key_val(self, df_target):    \n",
    "        keys = list()\n",
    "        vals = list()\n",
    "        for query in df_target.to_numpy():\n",
    "            if query is not np.nan:\n",
    "                for (key, val) in parse.parse_qsl(query):\n",
    "                    keys.append(key)\n",
    "                    vals.append(val)\n",
    "        keys = np.unique(keys, return_counts=False)\n",
    "        vals = np.unique(vals, return_counts=False)\n",
    "        return keys, vals\n",
    "    \n",
    "    def _pick_one(self, component, frequency):\n",
    "               \n",
    "        if random.random() <= frequency :\n",
    "            return component[ random.randrange(0,len(component)) ]\n",
    "        else:\n",
    "            return 'None'\n",
    "    \n",
    "    def _quote_query(self, d):\n",
    "        query = dict()\n",
    "        for key in d.keys():\n",
    "            query[key] = str(d[key]).replace('None','')          \n",
    "        return parse.urlencode(query, doseq=True)\n",
    "\n",
    "    def _quote(self, string):\n",
    "        return parse.quote(str(string).replace('None',''))\n",
    "\n",
    "\n",
    "# augmented data\n",
    "n_add = 10000 # for each class\n",
    "augment_data = data_augmentation(data_org, n_add=n_add)\n",
    "data_aug = augment_data.data_aug\n",
    "print('\\n==== augmented data:', data_aug.shape[0])\n",
    "data_aug.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== dataset mali\n",
      "train: 116141\n",
      "valid: 0\n",
      "test : 1173\n",
      "==== dataset mali_augment\n",
      "train: 155727\n",
      "valid: 0\n",
      "test : 1572\n",
      "==== dataset benign\n",
      "train: 0\n",
      "valid: 0\n",
      "test : 0\n",
      "==== dataset benign_augment\n",
      "train: 0\n",
      "valid: 0\n",
      "test : 0\n",
      "==== dataset defacement\n",
      "train: 94242\n",
      "valid: 0\n",
      "test : 951\n",
      "==== dataset defacement_augment\n",
      "train: 104142\n",
      "valid: 0\n",
      "test : 1051\n",
      "==== dataset malware\n",
      "train: 2438\n",
      "valid: 0\n",
      "test : 24\n",
      "==== dataset malware_augment\n",
      "train: 12333\n",
      "valid: 0\n",
      "test : 124\n",
      "==== dataset phishing\n",
      "train: 9083\n",
      "valid: 0\n",
      "test : 91\n",
      "==== dataset phishing_augment\n",
      "train: 18981\n",
      "valid: 0\n",
      "test : 191\n",
      "==== dataset spam\n",
      "train: 10381\n",
      "valid: 0\n",
      "test : 104\n",
      "==== dataset spam_augment\n",
      "train: 20273\n",
      "valid: 0\n",
      "test : 204\n"
     ]
    }
   ],
   "source": [
    "# split data: train / valid / test\n",
    "import os\n",
    "\n",
    "def split_train_test(data, ratio):\n",
    "    data = data.to_numpy()\n",
    "    n_train = math.ceil(len(data)*ratio[0])\n",
    "    n_valid = math.ceil(len(data)*ratio[1])\n",
    "    indx_shuffle = [ x for x in range(data.shape[0]) ]\n",
    "    random.shuffle(indx_shuffle)\n",
    "    shuffled = data[indx_shuffle]\n",
    "    train = shuffled[:n_train, :]\n",
    "    if n_valid > 0: \n",
    "        valid = shuffled[n_train:n_train+n_valid, :]    \n",
    "    else:\n",
    "        valid = np.empty((n_valid,2))\n",
    "    test = shuffled[n_train+n_valid:, :]\n",
    "    return train, valid, test\n",
    "\n",
    "ratio = [0.99, 0.0, 0.01]\n",
    "fname = 'mali'\n",
    "dir_save = './train_data'\n",
    "if not os.path.isdir(dir_save):\n",
    "    os.mkdir(dir_save)\n",
    "    \n",
    "for is_augment in [False, True]:\n",
    "    if is_augment: \n",
    "        fname += '_augment'\n",
    "        all_data = pd.concat([data_org, data_aug])\n",
    "    else:\n",
    "        all_data = data_org\n",
    "    \n",
    "    all_data = all_data.drop_duplicates(subset=['url'])\n",
    "    train, valid, test = split_train_test(all_data, ratio)\n",
    "\n",
    "    df_train = pd.DataFrame(train, columns=['url','label'])\n",
    "    df_valid = pd.DataFrame(valid, columns=['url','label'])\n",
    "    df_test = pd.DataFrame(test, columns=['url','label'])\n",
    "\n",
    "    df_train.to_csv(os.path.join(dir_save,'{}.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "    df_valid.to_csv(os.path.join(dir_save,'{}_valid.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "    df_test.to_csv(os.path.join(dir_save,'{}_test.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "\n",
    "    print('==== dataset {}'.format(fname))\n",
    "    print('train: {}'.format(df_train.shape[0]))\n",
    "    print('valid: {}'.format(df_valid.shape[0]))\n",
    "    print('test : {}'.format(df_test.shape[0]))\n",
    "\n",
    "class_name = {'benign': 0, 'defacement': 1, 'malware': 2, 'phishing':3, 'spam':4}\n",
    "for cname, label in class_name.items():\n",
    "    fname = cname\n",
    "    data_org_current = data_org[ data_org['class'] == label ]\n",
    "    data_aug_current = data_aug[ data_aug['class'] == label ]\n",
    "    for is_augment in [False, True]:\n",
    "        if is_augment: \n",
    "            fname += '_augment'\n",
    "            all_data = pd.concat([data_org_current, data_aug_current])\n",
    "        else:\n",
    "            all_data = data_org_current\n",
    "\n",
    "        all_data = all_data.drop_duplicates(subset=['url'])\n",
    "        train, valid, test = split_train_test(all_data, ratio)\n",
    "\n",
    "        df_train = pd.DataFrame(train, columns=['url','label'])\n",
    "        df_valid = pd.DataFrame(valid, columns=['url','label'])\n",
    "        df_test = pd.DataFrame(test, columns=['url','label'])\n",
    "\n",
    "        df_train.to_csv(os.path.join(dir_save,'{}.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "        df_valid.to_csv(os.path.join(dir_save,'{}_valid.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "        df_test.to_csv(os.path.join(dir_save,'{}_test.txt'.format(fname)), columns=['url'], header=None, index=False)\n",
    "\n",
    "        print('==== dataset {}'.format(fname))\n",
    "        print('train: {}'.format(df_train.shape[0]))\n",
    "        print('valid: {}'.format(df_valid.shape[0]))\n",
    "        print('test : {}'.format(df_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptlesson]",
   "language": "python",
   "name": "conda-env-ptlesson-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
